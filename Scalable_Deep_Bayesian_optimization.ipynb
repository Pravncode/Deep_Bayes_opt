{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ecd3da0-257f-4961-a90d-7d13df819d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from torch.utils.data import DataLoader, TensorDataset,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55f140-2c6e-4865-ba86-cceb6d845091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02ac11fe-ebe3-4b0e-9e9f-81640e0f4a1c",
   "metadata": {},
   "source": [
    "#Checking GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d82a54-0338-40a9-b758-06096ed1d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd8c23-c888-42ab-ac2a-6beeeb3a1543",
   "metadata": {},
   "source": [
    "#Matlab Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0283d1cd-37b9-4e6b-8d4a-7a32d12f1312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running MATLAB Version: 24.2.0.2863752 (R2024b) Update 5\n"
     ]
    }
   ],
   "source": [
    "import matlab.engine\n",
    "\n",
    "# Start MATLAB\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "# Get the MATLAB version\n",
    "version = eng.version()\n",
    "print(f\"Running MATLAB Version: {version}\")\n",
    "\n",
    "# Quit MATLAB\n",
    "eng.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4e624-58df-4c9a-97af-053b5eda50e7",
   "metadata": {},
   "source": [
    "#Initial Data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a9b407d-c5a3-4a33-b261-0f3cc323c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 56) (200, 200) (50, 56) (50, 200)\n"
     ]
    }
   ],
   "source": [
    "X=np.loadtxt('pcb_ini_200.txt')\n",
    "Y=np.loadtxt('y_ini_200.txt')\n",
    "x_test=np.loadtxt('pcb_ini_MO_test.txt')\n",
    "S_test=np.loadtxt('S11_MO_test.txt')\n",
    "G_test=np.loadtxt('Gain_MO_test.txt')\n",
    "y_test = np.concatenate((S_test,G_test), axis=1)\n",
    "print(X.shape,Y.shape,x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f82d539-6c81-491d-ba31-413eb4ef2f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([180, 56]) torch.Size([180, 200]) torch.Size([20, 56]) torch.Size([20, 200])\n"
     ]
    }
   ],
   "source": [
    "# Assuming X and Y are already defined\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "print(X_train.shape,y_train.shape,X_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5abdbd0-68a5-41cf-9fee-b031bb412839",
   "metadata": {},
   "source": [
    "#Surrogate-Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4385be8-d10d-4915-91a5-85e382c6963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BayesianRegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim=56, output_dim=200, hidden_dims=[913, 546,1284,1897,1259,302], drop_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.input_bn = nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "        # Dynamically create hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            self.hidden_layers.append(nn.Linear(prev_dim, dim))\n",
    "            self.hidden_layers.append(nn.BatchNorm1d(dim))\n",
    "            self.hidden_layers.append(nn.Dropout(drop_rate))\n",
    "            prev_dim = dim\n",
    "        \n",
    "        # Output heads\n",
    "        self.s11_head = nn.Linear(prev_dim, 100)\n",
    "        self.gain_head = nn.Linear(prev_dim, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_bn(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                x = F.selu(layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        s11 = self.s11_head(x)\n",
    "        gain = self.gain_head(x)\n",
    "        return torch.cat([s11, gain], dim=1)\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "  \n",
    "# Example usage\n",
    "model = BayesianRegressionNet()\n",
    "# print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9099789b-f91d-43dd-955a-891ef70899ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/3\n",
      "Fold 1 Validation Loss: 2.4591\n",
      "Fold 2/3\n",
      "Fold 2 Validation Loss: 2.8518\n",
      "Fold 3/3\n",
      "Fold 3 Validation Loss: 2.8321\n",
      "\n",
      "Training final model on full training set...\n",
      "\n",
      "Test Loss: 2.2953\n",
      "Average Cross-Validation Loss: 2.7143\n"
     ]
    }
   ],
   "source": [
    "def init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "# Training function\n",
    "# Train surrogate with early stopping\n",
    "def train_surrogate(x_train_fold, y_train_fold, input_dim, output_dim,\n",
    "                    num_epochs=500, patience=15, batch_size=64, device='cuda'):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    x_train_fold, y_train_fold = x_train_fold.to(device), y_train_fold.to(device)\n",
    "\n",
    "    model = BayesianRegressionNet(input_dim, output_dim).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    def weighted_loss(output, target, weights=[0.5, 0.5]):\n",
    "        s11_loss = F.smooth_l1_loss(output[:, :100], target[:, :100], beta=0.1)\n",
    "        gain_loss = F.smooth_l1_loss(output[:, 100:], target[:, 100:], beta=0.1)\n",
    "        return weights[0] * s11_loss + weights[1] * gain_loss\n",
    "\n",
    "    dataset = TensorDataset(x_train_fold, y_train_fold)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    train_set, val_set = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size * 2, shuffle=False)\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_weights = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = weighted_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += weighted_loss(outputs, targets).item() * inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_weights = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return model, best_loss\n",
    "\n",
    "# Configuration\n",
    "input_dim = 56\n",
    "output_dim = 200\n",
    "batch_size = 32\n",
    "num_epochs = 1000\n",
    "patience = 20\n",
    "k = 3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X, dtype=torch.float32)\n",
    "y_train = torch.tensor(Y, dtype=torch.float32)\n",
    "X_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# 3-Fold Cross-validation\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "val_losses = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    x_fold_train = X_train[train_idx]\n",
    "    y_fold_train = y_train[train_idx]\n",
    "\n",
    "    model, val_loss = train_surrogate(\n",
    "        x_fold_train, y_fold_train,\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        num_epochs=num_epochs,\n",
    "        patience=patience,\n",
    "        batch_size=batch_size,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Fold {fold + 1} Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Final training on full data\n",
    "print(\"\\nTraining final model on full training set...\")\n",
    "final_model, _ = train_surrogate(\n",
    "    X_train, y_train,\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    num_epochs=num_epochs,\n",
    "    patience=patience,\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Test evaluation\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = final_model(X_test.to(device))\n",
    "    s11_loss = F.smooth_l1_loss(preds[:, :100], y_test[:, :100].to(device), beta=0.1)\n",
    "    gain_loss = F.smooth_l1_loss(preds[:, 100:], y_test[:, 100:].to(device), beta=0.1)\n",
    "    test_loss = 0.5 * s11_loss.item() + 0.5 * gain_loss.item()\n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "\n",
    "print(f\"Average Cross-Validation Loss: {np.mean(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960eec5-d2d3-4d32-9c18-95d2a034f57e",
   "metadata": {},
   "source": [
    "# Design Space Bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4eb8188-bbb8-4a6f-9e66-973e57885cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_design_space_bounds():\n",
    "    pcb_width = 25  # mm \n",
    "    pcb_length = 30  # mm \n",
    "    half_width = pcb_width / 2\n",
    "    half_length = pcb_length / 2\n",
    "\n",
    "    conductor_ranges = {\n",
    "        'C1': {'x_start': 0.1, 'x_end_range': (-4, -1), 'y_span': (-half_length, half_length-1)},\n",
    "#         'C2': {'x_start_range': (-16, -7), 'x_thickness_range': (0.5, 2), 'y_span': (-half_length, half_length-3)},\n",
    "        'G1': {'x_start': 0.1, 'x_end': -half_width, 'y_start': -half_length, 'y_end_range': (-3, 10)},\n",
    "#         'G2': {'x_start': -half_width, 'x_thickness_range': (1, 16), 'y_end_range': (2, 8)}\n",
    "    }\n",
    "\n",
    "    grid_x_cells = 3 \n",
    "    grid_y_cells = 4 \n",
    "    patch_width_range = (2, 4)    # mm \n",
    "    patch_length_range = (2, 7)   # mm \n",
    "    offset_range = (-2,2)         # mm\n",
    "\n",
    "\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "\n",
    "\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "\n",
    "    # Adjust bounds slightly to avoid identical lower and upper bounds\n",
    "    epsilon = 1e-2\n",
    "\n",
    "    lower_bounds.extend([\n",
    "        conductor_ranges['C1']['x_start'] - epsilon, conductor_ranges['C1']['x_end_range'][0], \n",
    "        conductor_ranges['C1']['y_span'][0] - epsilon, conductor_ranges['C1']['y_span'][1] - epsilon,\n",
    "\n",
    "    ])\n",
    "    upper_bounds.extend([\n",
    "        conductor_ranges['C1']['x_start'] + epsilon, conductor_ranges['C1']['x_end_range'][1],\n",
    "        conductor_ranges['C1']['y_span'][0] + epsilon, conductor_ranges['C1']['y_span'][1] + epsilon,\n",
    "\n",
    "    ])\n",
    "\n",
    "    for i in range(grid_x_cells * grid_y_cells):\n",
    "        col = i % grid_x_cells\n",
    "        row = i // grid_x_cells\n",
    "        cell_width = half_width / grid_x_cells\n",
    "        cell_height = pcb_length / grid_y_cells\n",
    "        center_x = -half_width + cell_width * (col + 0.5)\n",
    "        center_y = -half_length + cell_height * (row + 0.5)\n",
    "\n",
    "        lower_bounds.extend([\n",
    "            max(center_x + offset_range[0] - patch_width_range[1] / 2, -half_width),\n",
    "            max(center_x + offset_range[0] + patch_width_range[0] / 2, -half_width),\n",
    "            max(center_y + offset_range[0] - patch_length_range[1] / 2, -half_length),\n",
    "            max(center_y + offset_range[0] + patch_length_range[0] / 2, -half_length)\n",
    "        ])\n",
    "        upper_bounds.extend([\n",
    "            min(center_x + offset_range[1] - patch_width_range[0] / 2, half_width),\n",
    "            min(center_x + offset_range[1] + patch_width_range[1] / 2, half_width),\n",
    "            min(center_y + offset_range[1] - patch_length_range[0] / 2, half_length),\n",
    "            min(center_y + offset_range[1] + patch_length_range[1] / 2, half_length)\n",
    "        ])\n",
    "\n",
    "    g1_y_start = conductor_ranges['G1']['y_start']\n",
    "    g1_y_end_min = conductor_ranges['G1']['y_end_range'][0]\n",
    "    g1_y_end_max = conductor_ranges['G1']['y_end_range'][1]\n",
    "\n",
    "    lower_bounds.extend([\n",
    "        conductor_ranges['G1']['x_start'] - epsilon, conductor_ranges['G1']['x_end'] - epsilon, \n",
    "        g1_y_start - epsilon, g1_y_end_min - epsilon,\n",
    "\n",
    "    ])\n",
    "    upper_bounds.extend([\n",
    "        conductor_ranges['G1']['x_start'] + epsilon, conductor_ranges['G1']['x_end'] +\n",
    "        epsilon, g1_y_start + epsilon, g1_y_end_max + epsilon,\n",
    "\n",
    "    ])\n",
    "\n",
    "    bounds = list(zip(lower_bounds, upper_bounds))\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cc44040-8e90-46c3-8db4-fcb2d746ff3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = get_design_space_bounds()\n",
    "np.shape(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bb37651-0e37-433c-8f29-a69fa6e0432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(S, G):\n",
    "    \n",
    "    # Define target values\n",
    "    target_S11 = -15.0  # We want S11 < -15 dB\n",
    "    target_gain = 3.0   # We want Gain > 3 dBi\n",
    "\n",
    "    # Compute S11 penalty (higher than -15 dB is penalized)\n",
    "    penalty_S11 = torch.clamp(S - target_S11, min=0)  # max(S11 - (-15), 0)\n",
    "    mean_penalty_S11 = torch.mean(penalty_S11, dim=1)  # Average over frequency\n",
    "\n",
    "    # Compute Gain penalty (lower than 3 dBi is penalized)\n",
    "    penalty_gain = torch.clamp(target_gain - G, min=0)  # max(3 - Gain, 0)\n",
    "    mean_penalty_gain = torch.mean(penalty_gain, dim=1)  # Average over frequency\n",
    "\n",
    "    # Combine both penalties with equal weight\n",
    "    total_penalty = 0.5 * mean_penalty_S11 + 0.4 * mean_penalty_gain\n",
    "\n",
    "    return total_penalty  # Lower fitness is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9720db89-4e9e-4910-a01a-ab0494445f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(model, X_test, num_samples=100, batch_size=32):\n",
    "    model.train()  # Enable MC dropout\n",
    "    num_test_samples = X_test.size(0)\n",
    "    device = X_test.device\n",
    "\n",
    "    mean = torch.zeros((num_test_samples, 200), device=\"cpu\")\n",
    "    variance = torch.zeros((num_test_samples, 200), device=\"cpu\")\n",
    "\n",
    "    for start in range(0, num_test_samples, batch_size):\n",
    "        end = min(start + batch_size, num_test_samples)\n",
    "        batch = X_test[start:end].to(device)\n",
    "\n",
    "        batch_predictions = torch.zeros((num_samples, end - start, 200), device=\"cpu\")\n",
    "        for i in range(num_samples):\n",
    "            with torch.no_grad():\n",
    "                batch_predictions[i] = model(batch).detach().cpu()\n",
    "\n",
    "        mean[start:end] = batch_predictions.mean(dim=0)  # Remove dropout scaling\n",
    "        variance[start:end] = batch_predictions.var(dim=0) + 1e-6  # Stabilized variance\n",
    "\n",
    "        del batch, batch_predictions  # Remove redundant torch.cuda.empty_cache()\n",
    "\n",
    "    return mean.to(device), variance.to(device)\n",
    "\n",
    "\n",
    "def mc_dropout_predict_samples(model, X_test, num_samples=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Return raw MC samples (for EI calculation).\n",
    "    Enables dropout while keeping BatchNorm in eval mode.\n",
    "    \"\"\"\n",
    "    def enable_dropout_eval_only(model):\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, torch.nn.Dropout):\n",
    "                m.train()\n",
    "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "                m.eval()\n",
    "\n",
    "    enable_dropout_eval_only(model)\n",
    "\n",
    "    num_test_samples = X_test.size(0)\n",
    "    device = X_test.device\n",
    "    output_dim = 200  # Update this if your model has a different output shape\n",
    "\n",
    "    # Store all MC samples on CPU (memory-efficient)\n",
    "    all_samples = torch.zeros((num_samples, num_test_samples, output_dim), device=\"cpu\")\n",
    "\n",
    "    for start in range(0, num_test_samples, batch_size):\n",
    "        end = min(start + batch_size, num_test_samples)\n",
    "        batch = X_test[start:end].to(device)\n",
    "\n",
    "        # Generate MC samples for the current batch\n",
    "        batch_samples = []\n",
    "        for _ in range(num_samples):\n",
    "            with torch.no_grad():\n",
    "                pred = model(batch).detach().cpu()\n",
    "            batch_samples.append(pred)\n",
    "\n",
    "        batch_samples = torch.stack(batch_samples)  # [num_samples, batch_size, output_dim]\n",
    "        all_samples[:, start:end, :] = batch_samples\n",
    "\n",
    "    return all_samples.to(device)\n",
    "\n",
    "\n",
    "def train_surrogate(X_train, y_train, input_dim=56, output_dim=200, num_epochs=500, patience=30, batch_size=32, device='cuda'):\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    \n",
    "    model = BayesianRegressionNet(input_dim, output_dim).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    train_set, val_set = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size * 2, shuffle=False)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item() * inputs.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1:3d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f70aac-3a41-44d6-8c33-76fe308d20ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "380d9768-81a4-40e8-9849-da0d229ac431",
   "metadata": {},
   "source": [
    "# EI-DE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7136c18e-da35-44d7-bf25-bd0721e73d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "from scipy.stats.qmc import LatinHypercube\n",
    "\n",
    "def lhs_sampling(samples, dims, device=\"cpu\"):\n",
    "    \"\"\"Generate Latin Hypercube Samples and convert to PyTorch tensor.\"\"\"\n",
    "    lhs = LatinHypercube(d=dims)  # Create LHS sampler for given dimensions\n",
    "    sample_points = lhs.random(n=samples)  # Generate LHS samples in [0,1]\n",
    "    return torch.tensor(sample_points, dtype=torch.float32, device=device)\n",
    "\n",
    "def simplified_DE_optimizer(\n",
    "    model, f_best, bounds, pcb_width, pcb_length,\n",
    "    num_samples=50, q=30, maxiter=100,\n",
    "    popsize=100, F_init=0.9, cr_init=0.9,\n",
    "    local_search=True, local_search_maxiter=20,\n",
    "    xi=0.05,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    batch_size=128,\n",
    "    stagnation_limit=40\n",
    "):\n",
    "    def min_obj(X_batch, adaptive_xi):\n",
    "        if X_batch.dim() == 1:\n",
    "            X_batch = X_batch.unsqueeze(0)\n",
    "\n",
    "        mc_samples = mc_dropout_predict_samples(model, X_batch, num_samples=num_samples)\n",
    "        S = mc_samples[..., :100]\n",
    "        G = mc_samples[..., 100:]\n",
    "        obj_values = objective_function(S, G)\n",
    "\n",
    "        mu = torch.mean(obj_values, dim=0)\n",
    "        sigma = torch.std(obj_values, dim=0)\n",
    "        sigma = torch.clamp(sigma, min=1e-3)\n",
    "\n",
    "        improvement = torch.clamp(f_best - mu - adaptive_xi, min=0)\n",
    "        Z = improvement / (sigma + 1e-6)\n",
    "        Z = torch.clamp(Z, min=-5, max=5)\n",
    "        normal = Normal(0, 1)\n",
    "        EI = improvement * normal.cdf(Z) + sigma * torch.exp(normal.log_prob(Z))\n",
    "        EI[sigma < 1e-3] = 0\n",
    "\n",
    "        return -EI.sum()\n",
    "\n",
    "    bounds_tensor = torch.tensor(bounds, dtype=torch.float32, device=device)\n",
    "    dim = bounds_tensor.shape[0]\n",
    "\n",
    "    population = lhs_sampling(popsize, dim, device=device)\n",
    "    population = population * (bounds_tensor[:, 1] - bounds_tensor[:, 0]) + bounds_tensor[:, 0]\n",
    "    population = check_and_adjust_coordinates_within_bounds(population, pcb_width, pcb_length)\n",
    "\n",
    "    fitness = torch.zeros(popsize, device=device)\n",
    "    for i in range(0, popsize, batch_size):\n",
    "        fitness[i:i+batch_size] = min_obj(population[i:i+batch_size], xi)\n",
    "\n",
    "    best_fitness = fitness.min()\n",
    "    best_params = population[fitness.argmin()]\n",
    "    stagnation_counter = 0\n",
    "\n",
    "    for generation in range(maxiter):\n",
    "        if stagnation_counter >= stagnation_limit:\n",
    "            print(f\"Early stopping at generation {generation + 1} due to stagnation.\")\n",
    "            break\n",
    "\n",
    "        # Adaptive F and CR: linearly decreasing/increasing\n",
    "        F = F_init * (1 - generation / maxiter)\n",
    "        cr = cr_init * (1 - generation / maxiter)\n",
    "        adaptive_xi = max(0.1, 0.5 * (stagnation_counter / stagnation_limit))\n",
    "\n",
    "        idx = torch.randint(0, popsize, (3, popsize), device=device)\n",
    "        a, b, c = idx[0], idx[1], idx[2]\n",
    "        mutant = population[a] + F * (population[b] - population[c])\n",
    "        mutant = torch.clamp(mutant, bounds_tensor[:, 0], bounds_tensor[:, 1])\n",
    "        mutant = check_and_adjust_coordinates_within_bounds(mutant, pcb_width, pcb_length)\n",
    "\n",
    "        cross_mask = torch.rand(popsize, dim, device=device) < cr\n",
    "        trial_pop = torch.where(cross_mask, mutant, population)\n",
    "        trial_pop = check_and_adjust_coordinates_within_bounds(trial_pop, pcb_width, pcb_length)\n",
    "\n",
    "        trial_fitness = torch.zeros(popsize, device=device)\n",
    "        for i in range(0, popsize, batch_size):\n",
    "            trial_fitness[i:i+batch_size] = min_obj(trial_pop[i:i+batch_size], adaptive_xi)\n",
    "\n",
    "        improved = trial_fitness < fitness\n",
    "        population[improved] = trial_pop[improved]\n",
    "        fitness[improved] = trial_fitness[improved]\n",
    "\n",
    "        current_best = fitness.min()\n",
    "        if current_best < best_fitness:\n",
    "            best_fitness = current_best\n",
    "            best_params = population[fitness.argmin()]\n",
    "            stagnation_counter = 0\n",
    "        else:\n",
    "            stagnation_counter += 1\n",
    "\n",
    "        if generation % 10 == 0 or generation == maxiter - 1:\n",
    "            print(f\"Generation {generation + 1}/{maxiter} | Best Fitness: {best_fitness:.6f} | Stagnation: {stagnation_counter}\")\n",
    "\n",
    "    # Local search on top q candidates\n",
    "    if local_search:\n",
    "        print(f\"Refining top {q} candidates with L-BFGS...\")\n",
    "        top_q_indices = torch.argsort(fitness)[:q]\n",
    "        selected_points = population[top_q_indices]\n",
    "\n",
    "        refined_candidates = []\n",
    "        for idx in range(q):\n",
    "            x = selected_points[idx].clone().unsqueeze(0).requires_grad_(True)\n",
    "\n",
    "            optimizer = torch.optim.LBFGS([x], max_iter=local_search_maxiter, line_search_fn='strong_wolfe')\n",
    "\n",
    "            \n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "                \n",
    "                enable_dropout_eval_only(model)\n",
    "            \n",
    "                loss = min_obj(x, adaptive_xi)\n",
    "            \n",
    "                if loss.requires_grad:\n",
    "                    loss.backward()\n",
    "            \n",
    "                return loss\n",
    "\n",
    "            optimizer.step(closure)\n",
    "\n",
    "            x.data = torch.clamp(x.data, bounds_tensor[:, 0], bounds_tensor[:, 1])\n",
    "            x.data = check_and_adjust_coordinates_within_bounds(x, pcb_width, pcb_length)\n",
    "            refined_candidates.append(x.detach().squeeze(0))\n",
    "\n",
    "        selected_points = torch.stack(refined_candidates)\n",
    "\n",
    "    else:\n",
    "        top_q_indices = torch.argsort(fitness)[:q]\n",
    "        selected_points = population[top_q_indices]\n",
    "\n",
    "    return selected_points.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559b807-ce07-403e-8937-ac3be7dff5bb",
   "metadata": {},
   "source": [
    "#Matlab functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f91af6b-d124-4f51-96f8-3f4b32c46d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcb_width = 25  # mm\n",
    "pcb_length = 30  # mm\n",
    "import matlab.engine\n",
    "\n",
    "def mirror_design_along_y(design, pcb_width, pcb_length):\n",
    "    \"\"\"\n",
    "    Mirrors the design along the y-axis.\n",
    "    :param design: Torch tensor containing the design of shape [q, 88].\n",
    "    :param pcb_width: Width of the PCB.\n",
    "    :param pcb_length: Length of the PCB.\n",
    "    :return: Torch tensor containing the mirrored design of shape [q, 88].\n",
    "    \"\"\"\n",
    "    mirrored_design = design.clone()\n",
    "    var_per_component = 4  # x_start, x_end, y_start, y_end\n",
    "    half_width = pcb_width / 2\n",
    "    half_length = pcb_length / 2\n",
    "\n",
    "    for i in range(0, design.shape[1], var_per_component):\n",
    "        x_start, x_end, y_start, y_end = design[:, i:i + var_per_component].T\n",
    "\n",
    "        # Mirror the x-coordinates along the y-plane\n",
    "        mirrored_x_start = -x_end\n",
    "        mirrored_x_end = -x_start\n",
    "\n",
    "        # Clamp mirrored x and y coordinates to ensure bounds\n",
    "        mirrored_x_start = torch.clamp(mirrored_x_start, -half_width, half_width)\n",
    "        mirrored_x_end = torch.clamp(mirrored_x_end, -half_width, half_width)\n",
    "        y_start = torch.clamp(y_start, -half_length, half_length)\n",
    "        y_end = torch.clamp(y_end, -half_length, half_length)\n",
    "\n",
    "        # Update mirrored design\n",
    "        mirrored_design[:, i:i + var_per_component] = torch.stack(\n",
    "            [mirrored_x_start, mirrored_x_end, y_start, y_end], dim=1)\n",
    "\n",
    "    return mirrored_design\n",
    "\n",
    "def extract_G1_G2_coords(design):\n",
    "    \"\"\"\n",
    "    Extracts G1 coordinates from the design array.\n",
    "    Assumes G1 is at indices [-4:].\n",
    "    \"\"\"\n",
    "    return design[:, -4:]  # Directly slice the last 4 columns\n",
    "\n",
    "\n",
    "def remove_redundant_variables(design):\n",
    "    \"\"\"\n",
    "    Removes G1 coordinates from the design array.\n",
    "    Assumes G1 is at indices [-4:].\n",
    "    \"\"\"\n",
    "    return design[:, :-4]\n",
    "\n",
    "\n",
    "def matlab_simulation_function(design):\n",
    "    \"\"\"\n",
    "    Calls the MATLAB simulation to predict VSWR for the given PCB design.\n",
    "    :param design: Torch tensor containing the design.\n",
    "    :return: Torch tensor with the simulated VSWR values.\n",
    "    \"\"\"\n",
    "    # Save the design to a text file, converting to CPU if necessary\n",
    "    design_numpy = design.cpu().numpy()\n",
    "    np.savetxt('combined_pcb_new.txt', design_numpy, delimiter='\\t')\n",
    "\n",
    "    # Start MATLAB engine and run the simulation script\n",
    "#     eng = matlab.engine.start_matlab()\n",
    "    eng.Ant_sim56(nargout=0)\n",
    "\n",
    "    # Load the VSWR result from the file generated by MATLAB\n",
    "    S11_result = np.loadtxt('S11_t.txt', delimiter='\\t')\n",
    "    gain_result = np.loadtxt('Gain_t.txt', delimiter='\\t')\n",
    "    \n",
    "    combined_results = np.concatenate((S11_result, gain_result), axis=1)\n",
    "\n",
    "    # Convert to torch tensor and return (on same device as input)\n",
    "    return torch.tensor(combined_results, dtype=torch.float32, device=design.device)\n",
    "\n",
    "def check_and_adjust_coordinates_within_bounds(combined_pcb_variables, pcb_width, pcb_length):\n",
    "    \"\"\"\n",
    "    Ensures all coordinates in the design are within the PCB bounds.\n",
    "    :param combined_pcb_variables: Torch tensor containing the combined design variables.\n",
    "    :param pcb_width: Width of the PCB.\n",
    "    :param pcb_length: Length of the PCB.\n",
    "    :return: Torch tensor with adjusted coordinates.\n",
    "    \"\"\"\n",
    "    half_width = torch.tensor(pcb_width / 2, dtype=combined_pcb_variables.dtype, device=combined_pcb_variables.device)\n",
    "    half_length = torch.tensor(pcb_length / 2, dtype=combined_pcb_variables.dtype, device=combined_pcb_variables.device)\n",
    "\n",
    "    adjusted_coords = combined_pcb_variables.clone()\n",
    "\n",
    "    for i in range(0, adjusted_coords.shape[1], 4):\n",
    "        # Extract and adjust x and y coordinates\n",
    "        x_start, x_end, y_start, y_end = adjusted_coords[:, i:i+4].T\n",
    "\n",
    "        x_start = torch.clamp(x_start, -half_width, half_width)\n",
    "        x_end = torch.clamp(x_end, -half_width, half_width)\n",
    "        y_start = torch.clamp(y_start, -half_length, half_length)\n",
    "        y_end = torch.clamp(y_end, -half_length, half_length)\n",
    "\n",
    "        # Update adjusted coordinates\n",
    "        adjusted_coords[:, i:i+4] = torch.stack([x_start, x_end, y_start, y_end], dim=1)\n",
    "\n",
    "    return adjusted_coords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d058b9-dfed-49c4-9687-8f37a3c2c956",
   "metadata": {},
   "source": [
    "# Bayesian Optimization Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c347ccb-f1e1-4af6-8364-aeb876a23d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_optimization(X_train, y_train, bounds, num_iterations=100, xi=0.05, num_samples=100, q=30):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    bounds = np.array(bounds)\n",
    "    input_dim = bounds.shape[0]\n",
    "\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "\n",
    "    # Load or train model (handled by train_surrogate)\n",
    "    model, _ = train_surrogate(X_train, y_train, input_dim=input_dim,output_dim=output_dim, device=device)\n",
    "    \n",
    "    best_design = None\n",
    "    best_objective_value = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    # Open files for saving best values\n",
    "    best_objective_file = open(\"best_objective_per_iteration.txt\", \"a\")\n",
    "    best_design_file = open(\"best_design_per_iteration.txt\", \"a\")\n",
    "    for iteration in range(num_iterations):\n",
    "        if no_improvement_count >= 15:\n",
    "            print(f\"Early stopping at iteration {iteration + 1}\")\n",
    "            break\n",
    "\n",
    "        # Calculate f_best from ALL historical data\n",
    "        all_scores = objective_function(y_train[:, :100], y_train[:, 100:])\n",
    "        f_best = all_scores.min()\n",
    "        print('Current f_best:', f_best.item())\n",
    "\n",
    "        # Get next samples using BEST model weights\n",
    "        next_samples = simplified_DE_optimizer(model, f_best.item(), bounds, popsize =popsize,pcb_width = pcb_width, pcb_length=pcb_length, \n",
    "                                             num_samples=num_samples, q=q, device=device)\n",
    "        next_samples_tensor = torch.tensor(next_samples, dtype=torch.float32, device=device)\n",
    "\n",
    "        combined_pcb_variables_list = []\n",
    "        for next_sample in next_samples_tensor:\n",
    "            mirrored_sample = mirror_design_along_y(next_sample.unsqueeze(0), pcb_width, pcb_length).to(device)\n",
    "            G1_original = extract_G1_G2_coords(next_sample.unsqueeze(0)).to(device)\n",
    "            G1_mirrored = extract_G1_G2_coords(mirrored_sample).to(device)\n",
    "            next_sample_trimmed = remove_redundant_variables(next_sample.unsqueeze(0)).to(device)\n",
    "            mirrored_sample_trimmed = remove_redundant_variables(mirrored_sample).to(device)\n",
    "            combined_pcb_variables = torch.cat((next_sample_trimmed, mirrored_sample_trimmed, G1_original, G1_mirrored), dim=1).to(device)\n",
    "            combined_pcb_variables = check_and_adjust_coordinates_within_bounds(\n",
    "                combined_pcb_variables, pcb_width, pcb_length\n",
    "            ).to(device)\n",
    "            combined_pcb_variables_list.append(combined_pcb_variables)\n",
    "        \n",
    "        all_combined_pcb_variables = torch.cat(combined_pcb_variables_list, dim=0).to(device)\n",
    "        next_samples_output = matlab_simulation_function(all_combined_pcb_variables).to(device).view(q, -1)\n",
    "\n",
    "        X_train = torch.cat((X_train, next_samples_tensor), dim=0).to(device)\n",
    "        y_train = torch.cat((y_train, next_samples_output), dim=0).to(device)\n",
    "\n",
    "        model, _ = train_surrogate(X_train, y_train, input_dim=input_dim, device=device)\n",
    "        \n",
    "\n",
    "        current_scores = objective_function(next_samples_output[:, :100], next_samples_output[:, 100:])\n",
    "        current_best_output = torch.tensor(current_scores, device=device).min().item()\n",
    "        # Save best objective and best design per iteration\n",
    "        best_objective_file.write(f\"{current_best_output:.6f}\\n\")\n",
    "        np.savetxt(best_design_file, next_samples_tensor.cpu().numpy()[:1], delimiter='\\t')\n",
    "\n",
    "        if current_best_output < best_objective_value:\n",
    "            best_objective_value = current_best_output\n",
    "            np.savetxt('best_design.txt', next_samples_tensor.cpu().numpy(), delimiter='\\t')\n",
    "            np.savetxt('best_design_output.txt', next_samples_output.cpu().numpy(), delimiter='\\t')\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "        \n",
    "        print(f'Iteration {iteration + 1}/{num_iterations} completed. Best objective value: {best_objective_value:.4f}')\n",
    "    best_objective_file.close()\n",
    "    best_design_file.close()\n",
    "    return model, X_train, y_train, best_design\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c0f7f9d-ea8b-43c5-8b76-281d461a7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "t1 = datetime.datetime.now()\n",
    "print(t1)\n",
    "bounds = get_design_space_bounds()\n",
    "maxiter = 100  \n",
    "popsize = 100\n",
    "input_dim=56\n",
    "output_dim = 200\n",
    "num_iterations = 100  \n",
    "q = 30\n",
    "num_samples = 100\n",
    "# model,_ = train_surrogate(X_train, y_train, input_dim=56, output_dim=200,num_epochs=500, patience=15, \n",
    "#                           batch_size=64, device=device)\n",
    "eng = matlab.engine.start_matlab()\n",
    "model, X_train, y_train, best_design  = bayesian_optimization(X_train, y_train, bounds, num_iterations=num_iterations, xi=0.05, num_samples=num_samples, q=q)\n",
    "print('X_train Shape:',X_train.shape,\"Best Design:\", best_design)\n",
    "t2 = datetime.datetime.now() - t1\n",
    "print('time_taken=',t2)\n",
    "eng.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02acb35-b164-4550-9899-0df7552e8465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
